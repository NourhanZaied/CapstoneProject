**Project Scope**

Build ETL pipeline that enables data integration strategies by gathering data from different resources and combine them into single resource by extracting data from original tables then transform data to Json files using spark and upload it to S3 then migrate it to Redshift on AWS in fact and dimension tables and finally will run data quality check to check any discrepancy on the data.

**Data gathering**

The datasets used and sources include:

- **I94 Immigration Data**: This data is retrieved from the US National Tourism and Trade Office and the source can be found [here](https://travel.trade.gov/research/reports/i94/historical/2016.html).
- **World Temperature Data**: This dataset came from Kaggle and you can find out about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).
- **U.S. City Demographic Data**: This data comes from OpenSoft [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).
- **Airport Code Table**: This is a simple table of airport codes and corresponding cities that is retrieved from [here](https://datahub.io/core/airport-codes#data)

**Use Cases:**

- Knowing how temperature changes affects city’s population.
- Analise these changes by grouping by genders.
- Determine traffic sizes during specific period.
- Identify how the temperature affect the number of immigrations for each city.
- Determine what type of visa immigrants have, what ports are the highest in traffic in specific period.
**


**Database Model**

Model will consist of one fact table and three dimension tables :

**City Fact Table**


|**Column Name**|**Data Type**|**Description**|
| :- | :- | :- |
|**city\_id (PRIMARY\_KEY)**|varchar(32)|auto generated primary key|
|**city\_name**|varchar(32)|name of city|
|**country**|varchar(32)|name of country|
|**latitude**|varchar(10)|latitude value|
|**longitude**|varchar(10)|longitude value|
|**average\_temperature**|numeric|average temperature of the city|
|**date**|date|date of temperature recording|
||||
**Airport Dimension Table**

|**Table Column**|**Data Type**|**Description**|
| :-: | :-: | :-: |
|**airport\_id (PRIMARY\_KEY)**|varchar(50)|auto generated primary key|
|**airport\_code**|varchar(50)|airport short code|
|**name**|varchar(500)|name of airport|
|**continent**|varchar(50)|continent code|
|**country\_code**|varchar(32)|country code|
|**state**|varchar(32)|state code|
**Immigrant Dimension Table**

|**Table Column**|**Data Type**|**Description**|
| :-: | :-: | :-: |
|**Immigrant\_id (PRIMARY\_KEY)**|varchar(32)|auto generated primary key|
|**year**|int4|year of visit|
|**month**|int4|month of visit|
|**city**|varchar(32)|city of visit|
|**gender**|varchar(32)|gender of visitor|
|**arrival\_date**|date|arrival date of the visitor|
|**departure\_date**|date|departure time of the visitor|
|**airline**|varchar(32)|airline code|


**Demographic Dimension Table**

|**Table Column**|**Data Type**|**Description**|
| :-: | :-: | :-: |
|**demographic\_id (PRIMARY\_KEY)**|varchar(100)|auto generated primary key|
|**city**|varchar(50)|city name|
|**state**|varchar(50)|state code|
|**male\_population**|int4|male population numbers by city|
|**female\_population**|int4|female population numbers by city|
|**total\_population**|int4|total population numbers by city|
**

**Model**

In this model I used the star schema with the fact and dimension tables. I used the city as the fact table because it’s related to all other tables from the different data sets which can be used to build the data pipeline.

**Queries**

- Find the average immigration traffic at a given city at a given port of entry by month.
- Find the average age of immigrants at a given port of entry by month and by airline/ or by gender.
- Retrieve the average population and average temperature of a given city.

**Tools and Technologies** 

- **Apache Spark** – 
  - Used to process data from the SAS and csv files to json data files. 
  - Convert some of the values such as the city codes from Label Descriptions data dictionary to city names in the data generated by spark. 
  - Write the processed data to S3.
- **Apache Airflow** – 
  - Schedule and monitor workflow.
  - Create In RedShift the staging, fact and dimension tables then copy json files from s3 to redshift staging table and preform ETL to load the final data to the fact and dimension tables.
- **Amazon Redshift** – 
  - Used to store the data from s3 into staging tables and then the data generated from the staging tables to fact and dimension.
  - Database that located in redshift cluster that stores data from s3.
- **Amazon S3** – Used to store the json files generated by spark.


**Dag View**

Uploaded in Project folder

**Process Steps**

1. Create a Redshift cluster with a redshift database.
1. Add AWS and Redshift credentials in the airflow UI. You can accomplish this in the following steps:
   1. Click on the **Admin tab** and select **Connections**.
   1. Under **Connections**, select **Create**.
   1. In the Create tab enter the following creds:
      1. Conn Id: aws\_credentials
      1. Conn Type: Amazon Web Services
      1. Login: Your <AWS Access Key ID>
      1. Password: <Your AWS Secret Access Key>
   1. Once done, click on **Save and Add Another**
   1. On the new create page add the following:
      1. Conn Id: redshift
      1. Conn Type: Postgres
      1. Host: <Endpointof your redshift cluster>
      1. Schema: <Redshift database name>
      1. Login: <Database username>
      1. Password: <Database password>
      1. Port: <Database port which is usually 5439>
   1. Click save

1. Get SAS immigration files and store them in a folder data/immigration-data. Get the GlobalLandTemperaturesByCity.csv file and store it in a folder data/temperature. Fill the configuration file by the bucket name, the input path of the data folder, the output path of the json files and the amazon id and secret key.
1. Run additional\_helpers/python data\_to\_json.py file, which will create a bucket and convert the source data into json files and upload it to the s3 bucket created.
1. Run the Capstone dag to create staging tables from s3 json file to redshift then it will process the data to fact and dim tables then quality checks the data.

**Suggestion for data update frequency**

Should be updated weekly or more frequently as it will help to produce information as the temperature changes, and daily for the traffic information, so it depends on the output of the data which will determine if weekly or daily.

**Possible Scenarios that may arise and how they can be handled.**

- If the data gets increased by 100x:
  - The increase of reads and writes to the database can be handled by increasing the number of compute nodes being used in the redshift cluster using elastic resize that can handle for more storage.
  - Use of distkeys in case of a need to join the tables.
  - Compress the s3 data.
- If the pipelines were needed to be run on a daily basis by 7am:
  - dags can be scheduled to run daily by setting the start\_date config as a datetime value containing both the date and time when it should start running, then setting schedule\_interval to @daily which will ensure the dag runs everyday at the time provided in start\_date.
- If the database needed to be accessed by 100+ people:
  - Utilizing elastic resize for better performance.
  - When you turn on concurrency scaling, Amazon Redshift automatically adds additional cluster capacity to process an increase in both read and write queries.


